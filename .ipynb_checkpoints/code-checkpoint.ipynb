{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b85f638c",
   "metadata": {},
   "source": [
    "# Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b32ef21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# if not installed, install using pip command. \n",
    "# See https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/ for more help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf14c1b",
   "metadata": {},
   "source": [
    "# Pre-processing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9faa290",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'within_150m_time.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-71154d91bc3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mqueries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'within_150m_time.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_queries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m20110000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_fwf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'paths(0:10996).csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minfer_nrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10997\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_replace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/env1/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/env1/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/env1/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/env1/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/env1/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/env1/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/env1/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'within_150m_time.csv'"
     ]
    }
   ],
   "source": [
    "queries = pd.read_csv('within_150m_time.csv')\n",
    "train_queries = queries[queries['time'] <= 20110000]\n",
    "paths = pd.read_fwf('paths(0:10996).csv', header=None,infer_nrows=10997)\n",
    "paths = paths[0].str.split(',', expand=True)\n",
    "paths.replace(to_replace=[None], value= -1, inplace=True)\n",
    "lst = train_queries.index.values.tolist()\n",
    "train_paths = paths[paths.index.isin(lst)]\n",
    "\n",
    "train_queries.reset_index(drop=True, inplace=True)\n",
    "train_paths.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train_queries.to_csv('train_queries.csv',index=False)\n",
    "train_paths.to_csv('train_paths.csv',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25868d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding duplicate paths\n",
    "\n",
    "paths = pd.read_fwf('train_paths.csv', header=None,infer_nrows=10997)\n",
    "paths = paths[0].str.split(',', expand=True)\n",
    "paths.replace(to_replace=[None], value= -1, inplace=True)\n",
    "queries = pd.read_csv('train_queries.csv') \n",
    "queries=queries.rename(columns = {'Unnamed: 0':'path_no'})\n",
    "\n",
    "paths = paths.apply(pd.to_numeric)\n",
    "a = (paths.values)\n",
    "len_path = []\n",
    "for j,path in enumerate(a):\n",
    "    l = np.count_nonzero(path+1)\n",
    "    len_path.append(l)\n",
    "    \n",
    "profit_path = 9614*[0]\n",
    "\n",
    "for i,path in enumerate(a):\n",
    "    profit = 0\n",
    "    if i>=0:          \n",
    "        max_node = (max(path[0],path[len_path[i]-1]))\n",
    "        min_node = (min(path[0],path[len_path[i]-1]))\n",
    "        c = []\n",
    "        for j,src_des in queries.iterrows():\n",
    "                src = min(src_des[0],src_des[1])\n",
    "                des = max(src_des[0],src_des[1])\n",
    "                if (des==max_node and src==min_node):\n",
    "                    c.append(j)\n",
    "          \n",
    "        profit  = min(c)\n",
    "        profit_path[profit] += 1\n",
    "        print(i,'of 0-all : ',profit)\n",
    "        \n",
    "                    \n",
    "pd.DataFrame(profit_path).to_csv('duplicates.csv',index=False,header=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9aac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing profits\n",
    "\n",
    "paths = pd.read_fwf('train_paths.csv', header=None,infer_nrows=10997)\n",
    "paths = paths[0].str.split(',', expand=True)\n",
    "paths.replace(to_replace=[None], value= -1, inplace=True)\n",
    "queries = pd.read_csv('train_queries.csv') \n",
    "\n",
    "\n",
    "paths = paths.apply(pd.to_numeric)\n",
    "a = (paths.values)\n",
    "a.sort(axis=1)\n",
    "a = a[:, ::-1]\n",
    "len_path = []\n",
    "for j,path in enumerate(a):\n",
    "    l = np.count_nonzero(path+1)\n",
    "    len_path.append(l)\n",
    "    \n",
    "profit_path = []\n",
    "\n",
    "for i,path in enumerate(a):\n",
    "    profit = 0\n",
    "    if i>=0:\n",
    "        max_node = path[0]\n",
    "        min_node = path[len_path[i]-1]\n",
    "        for j,src_des in queries.iterrows():\n",
    "            src = min(src_des[0],src_des[1])\n",
    "            des = max(src_des[0],src_des[1])\n",
    "            if des<=max_node and src>=min_node:\n",
    "                c = 0\n",
    "                for k in range(len_path[i]):\n",
    "                    if src == path[k]:\n",
    "                        c +=1\n",
    "                    if des == path[k]:\n",
    "                        c +=1\n",
    "                if c == 2:\n",
    "                    profit += 1\n",
    "\n",
    "        profit_path.append(profit)\n",
    "        print(i,'of 0-all profit : ',profit) \n",
    "                    \n",
    "pd.DataFrame(profit_path).to_csv('profit.csv',index=False,header=False) \n",
    "\n",
    "paths = pd.read_fwf('train_paths.csv', header=None,infer_nrows=4251)\n",
    "paths = paths[0].str.split(',', expand=True)\n",
    "paths.replace(to_replace=[None], value= -1, inplace=True)\n",
    "\n",
    "profit = pd.read_csv('profit(0:9613).csv',header = None)\n",
    "\n",
    "paths = paths.apply(pd.to_numeric)\n",
    "a = (paths.values)\n",
    "a.sort(axis=1)\n",
    "a = a[:, ::-1]\n",
    "len_path = []\n",
    "for j,path in enumerate(a):\n",
    "    l = np.count_nonzero(path+1)\n",
    "    len_path.append(l)\n",
    "    \n",
    "ratio_path = []\n",
    "for i,pro in profit.iterrows():\n",
    "    ratio_path.append(pro[0]/len_path[i])\n",
    "    \n",
    "pd.DataFrame(ratio_path).to_csv('ratio(0:9613).csv',index=False,header=False)\n",
    "\n",
    "pd.DataFrame(len_path).to_csv('length(0:9613).csv',index=False,header=False)\n",
    "\n",
    "ratio = pd.read_csv('ratio(0:9613).csv',header = None)\n",
    "profit = pd.read_csv('profit(0:9613).csv',header = None)\n",
    "length = pd.read_csv('length(0:9613).csv',header = None)\n",
    "\n",
    "paths = pd.read_fwf('train_paths.csv', header=None,infer_nrows=4251)\n",
    "paths = paths[0].str.split(',', expand=True)\n",
    "paths.replace(to_replace=[None], value= -1, inplace=True)\n",
    "paths = paths.apply(pd.to_numeric)\n",
    "a = (paths.values)\n",
    "a.sort(axis=1)\n",
    "a = a[:, ::-1]\n",
    "\n",
    "frames = [ratio, profit, length]\n",
    "merge = pd.concat(frames,ignore_index=True,axis = 1)\n",
    "\n",
    "merge.to_csv('ratio-profit-length(0:9613).csv',index=False,header=False)\n",
    "\n",
    "sorted_merge = merge.sort_values(by=[0], ascending=False)\n",
    "\n",
    "sorted_merge.to_csv('sorted-ratio-profit-length(0:9613).csv',header=False)\n",
    "\n",
    "sort = pd.read_csv(\"sorted(0:9613).csv\",header = None)\n",
    "sort.columns=['path_number','number_of_queries_mapped']\n",
    "sort = sort[0:5759]\n",
    "\n",
    "paths = pd.read_csv('train_paths.csv', header=None)\n",
    "a = (paths.values)\n",
    "a.sort(axis=1)\n",
    "a = a[:, ::-1]\n",
    "len_path = []\n",
    "for j,path in enumerate(a):\n",
    "    l = np.count_nonzero(path+1)\n",
    "    len_path.append(l)\n",
    "    \n",
    "queries = pd.read_csv('train_queries.csv') \n",
    "\n",
    "sorted_ratio = pd.read_csv('sorted-ratio-profit-length(0:9613).csv',header = None)\n",
    "sorted_ratio.columns = ['path_no','ratio','profit','length']\n",
    "\n",
    "lst = sort.path_number.values.tolist()\n",
    "newdf = sorted_ratio[sorted_ratio.path_no.isin(lst)]\n",
    "newdf=newdf.reset_index(drop=True)\n",
    "newdf.to_csv('sorted-ratio-profit-length(0:5758).csv',header = False,index = False)\n",
    "\n",
    "sort.to_csv('train_path_no_with_duplicate_paths_count(sorted).csv',header = None,index=None)\n",
    "\n",
    "duplicate_count = pd.read_csv('train_path_no_with_duplicate_paths_count(sorted).csv',header = None)\n",
    "duplicate_count.columns=['path_no','duplicate_count']\n",
    "\n",
    "ratio = pd.read_csv('sorted-ratio-profit-length(0:5758).csv',header = None)\n",
    "ratio.columns=['path_no','ratio','profit','length']\n",
    "\n",
    "queries = pd.read_csv('train_queries.csv')\n",
    "\n",
    "paths = pd.read_csv('train_paths.csv', header=None)\n",
    "a = (paths.values)\n",
    "a.sort(axis=1)\n",
    "a = a[:, ::-1]\n",
    "len_path = []\n",
    "for j,path in enumerate(a):\n",
    "    l = np.count_nonzero(path+1)\n",
    "    len_path.append(l)\n",
    "    \n",
    "lst = ratio.path_no.values.tolist()\n",
    "newdf = paths[paths.index.isin(lst)]\n",
    "newdf.to_csv('train_paths_without_duplicates.csv')\n",
    "\n",
    "a=pd.read_csv('train_paths_without_duplicates.csv')\n",
    "a=a.rename(columns = {'Unnamed: 0':'path_no'})\n",
    "\n",
    "ratio = pd.read_csv('sorted-ratio-profit-length(0:5758).csv',header = None)\n",
    "ratio.columns = ['path_no','ratio','profit','length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b2c397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cachelist(cache,sorted_ratio,cumulative_length):\n",
    "    capacity  = cache * cumulative_length\n",
    "    b = 0\n",
    "    a = 0\n",
    "    cache_pathlist = []\n",
    "    for i,data in sorted_ratio.iterrows():\n",
    "        cache_pathlist.append(data[0])\n",
    "        a += data[3]\n",
    "        b += 1\n",
    "        if a>=capacity:\n",
    "            break\n",
    "        \n",
    "    return cache_pathlist\n",
    "\n",
    "cache = 0.001\n",
    "cumulative_length = 0\n",
    "total_paths = 0\n",
    "for i,data in ratio.iterrows():\n",
    "    cumulative_length += data[3]\n",
    "    total_paths += 1\n",
    "for i in range(1000):\n",
    "    cache_pathlist = cachelist(cache,ratio,cumulative_length)\n",
    "    print(len(cache_pathlist)) \n",
    "    pd.DataFrame(cache_pathlist).to_csv('cached/cache_'+str(\"%0.3f\"% cache)+'_pathlist.csv',index=False,header=False)\n",
    "    cache +=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003424d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = pd.read_csv('within-150m-time.csv')\n",
    "test_queries = queries[(queries['time'] >= 20110000) & (queries['time'] <= 20120000)]\n",
    "test_queries.reset_index(drop=True, inplace=True)\n",
    "\n",
    "paths = pd.read_csv('train_paths_without_duplicates.csv')\n",
    "paths=paths.rename(columns = {'Unnamed: 0':'path_no'})\n",
    "\n",
    "ans =[]\n",
    "ans.append(0)\n",
    "cache = 0\n",
    "cache_id = 0\n",
    "for z in tqdm(range(1000)):\n",
    "    cache = cache+0.001\n",
    "    cache_pathlist = pd.read_csv('cached/cache_'+str(\"%0.3f\"% cache)+'_pathlist.csv',header = None)\n",
    "    y = cache_pathlist.shape[0]\n",
    "    cache_pathlist = cache_pathlist[cache_id:cache_pathlist.shape[0]]\n",
    "    cache_id = y\n",
    "    cache_list = cache_pathlist[0].values.tolist()\n",
    "    p = paths[paths.path_no.isin(cache_list)]\n",
    "    p = p.drop(['path_no'],axis =1)\n",
    "    p = p.apply(pd.to_numeric)\n",
    "    a = (p.values)\n",
    "    a.sort(axis=1)\n",
    "    a = a[:, ::-1]\n",
    "    len_path = []\n",
    "    for j,path in enumerate(a):\n",
    "        l = np.count_nonzero(path+1)\n",
    "        len_path.append(l)\n",
    "\n",
    "    answer = 0\n",
    "    indexes_to_drop=[]\n",
    "    for i,src_des in test_queries.iterrows():\n",
    "        des = max(src_des[0],src_des[1])\n",
    "        src = min(src_des[0],src_des[1])\n",
    "        for j,path in enumerate(a):\n",
    "            max_node = path[0]\n",
    "            min_node = path[len_path[j]-1]\n",
    "            if des<=max_node and src>=min_node:\n",
    "                c = 0\n",
    "                for k in range(len_path[j]):\n",
    "                    if src == path[k]:\n",
    "                        c +=1\n",
    "                    if des == path[k]:\n",
    "                        c +=1\n",
    "                if c == 2:\n",
    "                    print(i)\n",
    "                    answer +=1\n",
    "                    indexes_to_drop.append(i)\n",
    "                        \n",
    "                    break\n",
    "       \n",
    "    ans.append(answer+ans[z])\n",
    "    print(z,answer+ans[z])\n",
    "    test_queries.drop(test_queries.index[indexes_to_drop],inplace = True)\n",
    "\n",
    "    test_queries.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(ans)\n",
    "pd.DataFrame(ans[1:]).to_csv('cached/ans.csv',index = False,header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1249f964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cache =0.001*100\n",
    "x = [cache*i for i in range(1,len(ans)+1)]\n",
    "y = [(i/942)*100 for i in ans]\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('Percent(%) of Cache stored')\n",
    "plt.ylabel('Percent(%) of Queries answered')\n",
    "plt.title('2007-2010 trained, tested on 2011')\n",
    "plt.savefig('78910.eps',dpi = 600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
